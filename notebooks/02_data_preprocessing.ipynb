{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aba30edc"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to access video files and save processed frames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4db0174"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to mount Google Drive, and the first code cell provided in the notebook is designed to do exactly that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0089c9d5",
        "outputId": "38b0bd10-01f1-44f5-d595-b06faa15d976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Agregar la carpeta donde guardo los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd72ceb9"
      },
      "source": [
        "## Initialize Directories and Parameters\n",
        "\n",
        "### Subtask:\n",
        "Set up input and output directories for videos and processed frames, and define parameters like the desired frames per second (fps) and target resolution for normalization (e.g., 640x480).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "815eaf2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires initializing input/output directories, creating the output directory if it doesn't exist, and defining processing parameters such as frames per second and target resolution. This code block will perform all these initial setup steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a0b9ff",
        "outputId": "084c75fb-909f-4c5f-85f4-a06aa536bad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw video directory: /content/drive/MyDrive\n",
            "Processed frames output directory: /content/drive/MyDrive/processed\n",
            "Desired frames per second (fps): 10\n",
            "Target resolution for frames: (640, 480)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "raw_dir = Path(\"/content/drive/MyDrive/\")\n",
        "out_dir = Path(\"/content/drive/MyDrive/processed\")\n",
        "out_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "fps = 10  # frames por segundo deseado\n",
        "target_resolution = (640, 480) # width, height for normalization\n",
        "\n",
        "print(f\"Raw video directory: {raw_dir}\")\n",
        "print(f\"Processed frames output directory: {out_dir}\")\n",
        "print(f\"Desired frames per second (fps): {fps}\")\n",
        "print(f\"Target resolution for frames: {target_resolution}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9d00d82"
      },
      "source": [
        "## Iterate and Process Videos\n",
        "\n",
        "### Subtask:\n",
        "Loop through each video file in the raw directory to extract frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a7ac39e",
        "outputId": "a416145e-9002-4a6c-f056-45a470ac0632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame extraction and initial processing loop completed.\n"
          ]
        }
      ],
      "source": [
        "for vid in raw_dir.glob(\"*.mp4\"):\n",
        "    print(f\"Processing video: {vid.name}\")\n",
        "    cap = cv2.VideoCapture(str(vid))\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {vid.name}\")\n",
        "        continue\n",
        "\n",
        "    out_subdir = out_dir / vid.stem\n",
        "    out_subdir.mkdir(exist_ok=True)\n",
        "    print(f\"  Output directory for frames: {out_subdir}\")\n",
        "\n",
        "    i = 0\n",
        "    frames_extracted_count = 0\n",
        "    video_fps_actual = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    if video_fps_actual == 0:\n",
        "        print(f\"Warning: Could not get FPS for {vid.name}, assuming 30 FPS for processing.\")\n",
        "        video_fps_actual = 30 # Default to 30 FPS if not available\n",
        "\n",
        "    skip_interval = max(1, int(video_fps_actual // fps))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if i % skip_interval == 0:\n",
        "            # Convert to grayscale and normalize resolution\n",
        "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            resized_frame = cv2.resize(gray_frame, target_resolution)\n",
        "\n",
        "            cv2.imwrite(str(out_subdir / f\"frame_{i:04d}.png\"), resized_frame)\n",
        "            frames_extracted_count += 1\n",
        "        i += 1\n",
        "    cap.release()\n",
        "    print(f\"  Extracted and processed {frames_extracted_count} frames from {vid.name}\")\n",
        "\n",
        "print(\"Frame extraction and initial processing loop completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48eafac9"
      },
      "source": [
        "## Stabilize Frames\n",
        "\n",
        "### Subtask:\n",
        "Implement video stabilization techniques using OpenCV to correct for unwanted camera movements, ensuring a smoother sequence of frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e90732c",
        "outputId": "62d10ed6-4598-49eb-f298-79b5b51de749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing for stabilization...\n",
            "Preparation for stabilization complete. Loaded frames into 'video_frames_to_stabilize' dictionary.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Placeholder for a list to store all frames for a video for stabilization\n",
        "video_frames_to_stabilize = {}\n",
        "\n",
        "print(\"Preparing for stabilization...\")\n",
        "\n",
        "for vid_subdir in out_dir.iterdir():\n",
        "    if vid_subdir.is_dir():\n",
        "        stabilized_subdir = vid_subdir / \"stabilized\"\n",
        "        stabilized_subdir.mkdir(exist_ok=True)\n",
        "        print(f\"  Created stabilized directory: {stabilized_subdir}\")\n",
        "\n",
        "        # Load all grayscale and normalized PNG frames for this video\n",
        "        frames_paths = sorted(vid_subdir.glob(\"frame_*.png\"))\n",
        "        if not frames_paths:\n",
        "            print(f\"    No frames found in {vid_subdir}, skipping stabilization for this video.\")\n",
        "            continue\n",
        "\n",
        "        current_video_frames = []\n",
        "        for frame_path in frames_paths:\n",
        "            frame = cv2.imread(str(frame_path), cv2.IMREAD_GRAYSCALE)\n",
        "            if frame is not None:\n",
        "                current_video_frames.append(frame)\n",
        "\n",
        "        if current_video_frames:\n",
        "            video_frames_to_stabilize[vid_subdir.name] = current_video_frames\n",
        "            print(f\"    Loaded {len(current_video_frames)} frames for stabilization from {vid_subdir.name}.\")\n",
        "        else:\n",
        "            print(f\"    Failed to load any frames from {vid_subdir}, skipping stabilization for this video.\")\n",
        "\n",
        "print(\"Preparation for stabilization complete. Loaded frames into 'video_frames_to_stabilize' dictionary.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "667c64de",
        "outputId": "7b339bff-4c3c-4359-82d4-67360cd993d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating raw camera motion...\n",
            "Raw camera motion calculation complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating raw camera motion...\")\n",
        "\n",
        "# Parameters for feature detection (goodFeaturesToTrack)\n",
        "feature_params = dict( maxCorners = 100,\n",
        "                       qualityLevel = 0.3,\n",
        "                       minDistance = 7,\n",
        "                       blockSize = 7 )\n",
        "\n",
        "# Parameters for optical flow (calcOpticalFlowPyrLK)\n",
        "lk_params = dict( winSize  = (15,15),\n",
        "                  maxLevel = 2,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "# Dictionary to store raw transformation matrices for each video\n",
        "video_raw_transforms = {}\n",
        "\n",
        "for video_name, frames in video_frames_to_stabilize.items():\n",
        "    if len(frames) < 2:\n",
        "        print(f\"  Skipping raw motion calculation for {video_name}: Less than 2 frames available.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  Calculating raw transformations for {video_name} ({len(frames)} frames)...\")\n",
        "    raw_transforms = []\n",
        "\n",
        "    # The first frame has no preceding transformation, so we add an identity matrix\n",
        "    # This will be used to accumulate motion later.\n",
        "    raw_transforms.append(np.array([[1., 0., 0.], [0., 1., 0.]], dtype=np.float32))\n",
        "\n",
        "    for i in range(len(frames) - 1):\n",
        "        prev_frame = frames[i]\n",
        "        curr_frame = frames[i+1]\n",
        "\n",
        "        # 1. Detect features in the previous frame\n",
        "        prev_pts = cv2.goodFeaturesToTrack(prev_frame, mask = None, **feature_params)\n",
        "\n",
        "        if prev_pts is None or len(prev_pts) < 10: # Ensure enough features are detected\n",
        "            # If not enough features, use a default identity transformation or log an error\n",
        "            transform = np.array([[1., 0., 0.], [0., 1., 0.]], dtype=np.float32)\n",
        "            raw_transforms.append(transform)\n",
        "            # print(f\"    Warning: Not enough features for frame {i} in {video_name}. Using identity transform.\")\n",
        "            continue\n",
        "\n",
        "        # 2. Track features to the current frame using optical flow\n",
        "        curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_frame, curr_frame, prev_pts, None, **lk_params)\n",
        "\n",
        "        # Filter out points that were not found in the current frame\n",
        "        good_prev_pts = prev_pts[status == 1]\n",
        "        good_curr_pts = curr_pts[status == 1]\n",
        "\n",
        "        if len(good_prev_pts) < 10 or len(good_curr_pts) < 10: # Ensure enough tracked features\n",
        "            transform = np.array([[1., 0., 0.], [0., 1., 0.]], dtype=np.float32)\n",
        "            raw_transforms.append(transform)\n",
        "            # print(f\"    Warning: Not enough tracked features for frame {i} in {video_name}. Using identity transform.\")\n",
        "            continue\n",
        "\n",
        "        # 3. Estimate the affine transformation between the two sets of points\n",
        "        # We need to reshape points for estimateAffine2D if they are not already (N, 1, 2)\n",
        "        # ensure good_prev_pts and good_curr_pts are float32\n",
        "        m, _ = cv2.estimateAffine2D(good_prev_pts, good_curr_pts)\n",
        "\n",
        "        if m is None: # If estimation fails, use identity matrix\n",
        "            m = np.array([[1., 0., 0.], [0., 1., 0.]], dtype=np.float32)\n",
        "            # print(f\"    Warning: Failed to estimate transform for frame {i} in {video_name}. Using identity transform.\")\n",
        "\n",
        "        # Store the transformation from previous to current frame\n",
        "        raw_transforms.append(m)\n",
        "\n",
        "    video_raw_transforms[video_name] = raw_transforms\n",
        "    print(f\"  Finished calculating {len(raw_transforms)} raw transformations for {video_name}.\")\n",
        "\n",
        "print(\"Raw camera motion calculation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe1f14b",
        "outputId": "b138015c-eb77-418a-90ba-24a55fb5820b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Smoothing camera motion...\n",
            "Camera motion smoothing complete. Stored in 'video_smoothed_transforms' dictionary.\n"
          ]
        }
      ],
      "source": [
        "print(\"Smoothing camera motion...\")\n",
        "\n",
        "# Smoothing window size (e.g., 30 frames for a 1-second smoothing at 30fps)\n",
        "SMOOTHING_RADIUS = 15 # Can be adjusted based on desired smoothness vs. lag\n",
        "\n",
        "# Dictionary to store smoothed transformation matrices for each video\n",
        "video_smoothed_transforms = {}\n",
        "\n",
        "# Function to smooth a path\n",
        "def smooth(trajectory, radius):\n",
        "    window_size = 2 * radius + 1\n",
        "    # Apply a moving average filter\n",
        "    s = np.convolve(trajectory, np.ones(window_size)/window_size, mode='valid')\n",
        "    # Pad the beginning and end to match the original trajectory length\n",
        "    pad_start = trajectory[0] - s[0] if len(s) > 0 else 0\n",
        "    pad_end = trajectory[-1] - s[-1] if len(s) > 0 else 0\n",
        "\n",
        "    # Calculate the average of the first 'radius' elements for the start padding\n",
        "    start_avg = np.mean(trajectory[:radius]) if radius > 0 else trajectory[0]\n",
        "    # Calculate the average of the last 'radius' elements for the end padding\n",
        "    end_avg = np.mean(trajectory[-radius:]) if radius > 0 else trajectory[-1]\n",
        "\n",
        "    # Pad the smoothed trajectory with the first/last values to match length\n",
        "    if len(s) == 0: # Handle case where trajectory is shorter than window\n",
        "        return trajectory\n",
        "\n",
        "    smoothed_trajectory = np.concatenate(\n",
        "        [np.full(radius, start_avg), s, np.full(radius, end_avg)]\n",
        "    )\n",
        "    return smoothed_trajectory\n",
        "\n",
        "\n",
        "for video_name, raw_transforms in video_raw_transforms.items():\n",
        "    if len(raw_transforms) < 2: # At least one actual transform (meaning 2 frames) + identity for first frame\n",
        "        print(f\"  Skipping smoothing for {video_name}: Not enough raw transformations.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  Processing {len(raw_transforms)} transformations for {video_name}...\")\n",
        "\n",
        "    # 1. Accumulate individual frame-to-frame transformations to get a global motion path\n",
        "    # This path represents the raw camera movement in terms of x, y, and rotation components.\n",
        "    x_path = [0.0]\n",
        "    y_path = [0.0]\n",
        "    a_path = [0.0] # Angle\n",
        "\n",
        "    for i in range(1, len(raw_transforms)): # Start from the first actual transform\n",
        "        dx = raw_transforms[i][0, 2]\n",
        "        dy = raw_transforms[i][1, 2]\n",
        "        da = np.arctan2(raw_transforms[i][0, 1], raw_transforms[i][0, 0]) # Rotation component\n",
        "\n",
        "        x_path.append(x_path[-1] + dx)\n",
        "        y_path.append(y_path[-1] + dy)\n",
        "        a_path.append(a_path[-1] + da)\n",
        "\n",
        "    # Convert paths to numpy arrays for smoothing\n",
        "    x_path = np.array(x_path)\n",
        "    y_path = np.array(y_path)\n",
        "    a_path = np.array(a_path)\n",
        "\n",
        "    # 2. Apply a smoothing filter to the global motion path\n",
        "    smoothed_x = smooth(x_path, SMOOTHING_RADIUS)\n",
        "    smoothed_y = smooth(y_path, SMOOTHING_RADIUS)\n",
        "    smoothed_a = smooth(a_path, SMOOTHING_RADIUS)\n",
        "\n",
        "    # 3. Calculate the correction transformation for each frame\n",
        "    # The difference between the raw path and the smoothed path gives the required correction\n",
        "    corrections = []\n",
        "    for i in range(len(raw_transforms)): # Iterate through all frames to apply corrections\n",
        "        # Desired_transform = Actual_transform * Correction_transform\n",
        "        # Correction_transform = Desired_transform * Actual_transform_inverse\n",
        "\n",
        "        # The desired frame-to-frame transform is derived from the smoothed path\n",
        "        # smoothed_dx = smoothed_x[i] - smoothed_x[i-1] (for i > 0)\n",
        "        # smoothed_dy = smoothed_y[i] - smoothed_y[i-1]\n",
        "        # smoothed_da = smoothed_a[i] - smoothed_a[i-1]\n",
        "\n",
        "        # The correction is the difference between the smoothed path and the raw path.\n",
        "        # This is the amount of motion to subtract from the raw motion.\n",
        "        dx_diff = smoothed_x[i] - x_path[i]\n",
        "        dy_diff = smoothed_y[i] - y_path[i]\n",
        "        da_diff = smoothed_a[i] - a_path[i]\n",
        "\n",
        "        # Construct the correction matrix\n",
        "        correction_matrix = np.array([\n",
        "            [np.cos(da_diff), -np.sin(da_diff), dx_diff],\n",
        "            [np.sin(da_diff),  np.cos(da_diff), dy_diff]\n",
        "        ], dtype=np.float32)\n",
        "        corrections.append(correction_matrix)\n",
        "\n",
        "    video_smoothed_transforms[video_name] = corrections\n",
        "    print(f\"  Generated {len(corrections)} correction transformations for {video_name}.\")\n",
        "\n",
        "print(\"Camera motion smoothing complete. Stored in 'video_smoothed_transforms' dictionary.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e486d8b",
        "outputId": "4ca09b14-76d4-4230-b58b-52a7225995d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying smoothed motion and saving stabilized frames...\n",
            "Stabilization and saving of frames complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Applying smoothed motion and saving stabilized frames...\")\n",
        "\n",
        "for video_name, frames in video_frames_to_stabilize.items():\n",
        "    if video_name not in video_smoothed_transforms:\n",
        "        print(f\"  Skipping stabilization for {video_name}: No smoothed transforms found.\")\n",
        "        continue\n",
        "\n",
        "    corrections = video_smoothed_transforms[video_name]\n",
        "\n",
        "    if len(frames) != len(corrections): # It should be frames + 1 for raw_transforms, but for corrections it should match frames count\n",
        "        print(f\"  Warning: Number of frames ({len(frames)}) does not match number of corrections ({len(corrections)}) for {video_name}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Get the output subdirectory for stabilized frames for this video\n",
        "    # out_dir / video_name is the video's original processed frame directory\n",
        "    # and inside that, there should be a 'stabilized' folder\n",
        "    stabilized_subdir = out_dir / video_name / \"stabilized\"\n",
        "    if not stabilized_subdir.exists():\n",
        "        stabilized_subdir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"  Created missing stabilized directory: {stabilized_subdir}\")\n",
        "\n",
        "    print(f\"  Stabilizing and saving frames for {video_name}...\")\n",
        "    stabilized_frames_count = 0\n",
        "    for i, frame in enumerate(frames):\n",
        "        # Apply the correction transformation\n",
        "        # cv2.warpAffine expects a 2x3 matrix\n",
        "        corrected_frame = cv2.warpAffine(frame, corrections[i], (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "        # Save the stabilized frame\n",
        "        cv2.imwrite(str(stabilized_subdir / f\"frame_stabilized_{i:04d}.png\"), corrected_frame)\n",
        "        stabilized_frames_count += 1\n",
        "    print(f\"  Saved {stabilized_frames_count} stabilized frames for {video_name}.\")\n",
        "\n",
        "print(\"Stabilization and saving of frames complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
